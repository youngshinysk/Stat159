curve(dgamma(x, shape = mle_alpha, rate = mle_lambda), add = TRUE, col = "blue", lwd = 2)
curve(dgamma(x, shape = mom_alpha, rate = mom_lambda), add = TRUE, col = "red", lwd = 2)
legend("topright", c("MOM"), lty=1, lwd=2.5, col=c("red"))
# Plot histogram
hist(arrivals, freq = FALSE, main = "Arrival Times for MLE",
xlab = "Arrival times", ylim = c(0, 0.013))
# Adding the gamma curve
curve(dgamma(x, shape = mle_alpha, rate = mle_lambda), add = TRUE, col = "blue", lwd = 2)
legend("topright", c("MLE"), lty=1, lwd=2.5, col=c("blue"))
# plot histogram
hist(arrivals, freq = FALSE, main = "Arrival Times for Both MOM and MLE",
xlab = "Arrival times", ylim = c(0, 0.013))
# Adding both of the gamma curve
curve(dgamma(x, shape = mom_alpha, rate = mom_lambda), add = TRUE, col = "red", lwd = 2)
curve(dgamma(x, shape = mle_alpha, rate = mle_lambda), add = TRUE, col = "blue", lwd = 2)
legend("topright", c("MOM","MLE"), lty=c(1,1), lwd=c(2.5,2.5), col=c("red","blue"))
func_alphahat<-function(x){mean(x)*mean(x)/(var(x)*(length(x)-1)/length(x))}
func_lambdahat<-function(x){mean(x)/(var(x)*(length(x)-1)/length(x))}
bootstrap_mom_alpha <- apply(bootstrap_mom_sample, 2, func_alphahat)
bootstrap_mom_lambda <- apply(bootstrap_mom_sample, 2, func_lambdahat)
bootstrap_mom_sample <- replicate(500, rgamma(3935, shape = mom_alpha, rate = mom_lambda))
bootstrap_mle_sample <- replicate(500, rgamma(3935, shape = mle_alpha,rate = mle_lambda))
#Functions for calculating alpha and lambda
func_alphahat<-function(x){mean(x)*mean(x)/(var(x)*(length(x)-1)/length(x))}
func_lambdahat<-function(x){mean(x)/(var(x)*(length(x)-1)/length(x))}
bootstrap_mom_alpha <- apply(bootstrap_mom_sample, 2, func_alphahat)
bootstrap_mom_lambda <- apply(bootstrap_mom_sample, 2, func_lambdahat)
boostrapt_mles <- apply(bootstrap_mle_sample, 2, function(x){fitdistr(x, "gamma")$estimate})
estimatedSE <- matrix(c(sd(bootstrap_mom_alpha), sd(bootstrap_mom_lambda), sd(boot_mles[1,]), sd(boot_mles[2,])),
suppressWarnings(boostrap_mles <- apply(bootstrap_mle_sample, 2, function(x){fitdistr(x, "gamma")$estimate}))
```
suppressWarnings(boostrap_mles <- apply(bootstrap_mle_sample, 2, function(x){fitdistr(x, "gamma")$estimate}))
estimatedSE <- matrix(c(sd(bootstrap_mom_alpha), sd(bootstrap_mom_lambda),
sd(bootstrap_mles[1,]), sd(boot_strapmles[2,])), nrow = 2, ncol = 2, byrow = TRUE,
dimnames = list(c("MOM", "MLE"), c("SE for alpha", "SE for lambda")))
suppressWarnings(bootstrap_mles <- apply(bootstrap_mle_sample, 2, function(x){fitdistr(x, "gamma")$estimate}))
estimatedSE <- matrix(c(sd(bootstrap_mom_alpha), sd(bootstrap_mom_lambda),
sd(bootstrap_mles[1,]), sd(bootstrap_strapmles[2,])), nrow = 2, ncol = 2, byrow = TRUE,
dimnames = list(c("MOM", "MLE"), c("SE for alpha", "SE for lambda")))
sd(bootstrap_mles[1,]), sd(bootstrap_mles[2,])), nrow = 2, ncol = 2, byrow = TRUE,
estimatedSE <- matrix(c(sd(bootstrap_mom_alpha), sd(bootstrap_mom_lambda),
sd(bootstrap_mles[1,]), sd(bootstrap_mles[2,])), nrow = 2, ncol = 2, byrow = TRUE,
dimnames = list(c("MOM", "MLE"), c("SE for alpha", "SE for lambda")))
estimatedSE
hist(boot_mom_alpha, main = "Bootstrap MOM: shape", xlab = expression(alpha))
abline(v = mom_alpha, col = "red")
hist(boot_mom_lambda, main = "Bootstrap MOM: rate", xlab = expression(lambda))
abline(v = mom_lambda, col = "red")
#MLE
par(mfrow=c(2, 2))
#MOM
hist(bootstrap_mom_alpha, main = "Bootstrap MOM: shape", xlab = expression(alpha))
abline(v = mom_alpha, col = "red")
hist(bootstrap_mom_lambda, main = "Bootstrap MOM: rate", xlab = expression(lambda))
abline(v = mom_lambda, col = "red")
#MLE
hist(bootstrap_mles[1,], main = "Bootstrap MLEs: shape", xlab = expression(alpha))
abline(v = mle_alpha, col = "red")
hist(bootstrap_mles[2,], main = "Bootstrap MLEs: rate", xlab = expression(lambda))
abline(v = mle_lambda, col = "red")
alpha_CI
lambda_CI
# shape (alpha)
mom_alpha_deltas <- quantile(boot_mom_alpha, probs = c(0.975, 0.025))
mom_alpha_CI <- 2 * mom_alpha - mom_a_deltas
# rate (lambda)
mom_lambda_deltas <- quantile(boot_mom_lambda, probs = c(0.975, 0.025))
mom_alpha_deltas <- quantile(bootstrap_mom_alpha, probs = c(0.975, 0.025))
mom_alpha_CI <- 2 * mom_alpha - mom_a_deltas
# rate (lambda)
mom_alpha_deltas <- quantile(bootstrap_mom_alpha, probs = c(0.975, 0.025))
mom_alpha_CI <- 2 * mom_alpha - mom_alpha_deltas
# rate (lambda)
mom_lambda_deltas <- quantile(bootstrap_mom_lambda, probs = c(0.975, 0.025))
mom_lambda_CI <- 2 * mom_lambda - mom_lambda_deltas
#CI for MLE estimates
# shape (alpha)
mle_alpha_deltas <- quantile(bootstrap_mles[1,], probs = c(0.975, 0.025))
mle_alpha_CI <- 2 * mle_alpha - mle_alpha_deltas
# rate (lambda)
mle_lambda_deltas <- quantile(bootstrap_mles[2,], probs = c(0.975, 0.025))
mle_lambda_CI <- 2 * mle_lambda - mle_lambda_deltas
alpha_CI <- matrix(c(mom_a_CI, mle_a_CI), nrow = 2, ncol = 2, byrow = TRUE,
dimnames = list(c("MOM", "MLE"), c("Lower Bound", "Upper Bound")))
alpha_CI <- matrix(c(mom_alpha_CI, mle_alpha_CI), nrow = 2, ncol = 2, byrow = TRUE,
dimnames = list(c("MOM", "MLE"), c("Lower Bound", "Upper Bound")))
lambda_CI <- matrix(c(mom_lambda_CI, mle_lambda_CI), nrow = 2, ncol = 2, byrow = TRUE,
dimnames = list(c("MOM", "MLE"), c("Lower Bound", "Upper Bound")))
#Confidence Intervals
alpha_CI
lambda_CI
qf(0.95, 9, 9)
sd(type1)
sd(type2)
type1 = c(3.03, 5.53, 5.60, 9.30, 9.92, 12.51, 12.95, 15.21, 16.04, 16.84)
type2 = c(3.19, 4.26, 4.47, 4.53, 4.67, 4.69, 12.78, 6.79, 9.37, 12.75)
qf(0.95, 9, 9)
sd(type1)
sd(type2)
var.test(type1, type2)
mean(a)
a <- c(3.03, 5.53, 5.60, 9.30, 9.92, 12.51, 12.95, 15.21, 16.04, 16.84)
b <- c(3.19, 4.26, 4.47, 4.53, 4.67, 4.69, 12.78, 6.79, 9.37, 12.75)
var.test(a, b)
mean(a)
mean(b)
sd(a)
sd(b)
a <- c(3.03, 5.53, 5.60, 9.30, 9.92, 12.51, 12.95, 15.21, 16.04, 16.84)
b <- c(3.19, 4.26, 4.47, 4.53, 4.67, 4.69, 12.78, 6.79, 9.37, 12.75)
var.test(a, b)
t.test(a, b, var.equal = TRUE, paired = FALSE)
wilcox.test(a, b)
qqnorm(a, main = "Type I", xlab = "Theoretical N(0,1) Quantiles", ylab = "Samples from Type I")
qqline(a)
qqnorm(b, main = "Type II", xlab = "Theoretical N(0,1) Quantiles",ylab = "Samples from Type II")
qqline(b)
par(mfrow=c(1, 2))
qqnorm(a, main = "Type I", xlab = "Theoretical N(0,1) Quantiles", ylab = "Samples from Type I")
qqline(a)
qqnorm(b, main = "Type II", xlab = "Theoretical N(0,1) Quantiles",ylab = "Samples from Type II")
qqline(b)
test <- c(676, 206, 230, 256, 280, 433, 337, 466, 497, 512, 794, 428, 452, 512)
control <- c(88, 570, 605, 617, 653, 2913, 924, 286, 1098, 982, 2346, 321, 615, 519)
difference <- test - control
difference
difference
plot(control, difference)
plot(control, difference, main = "control vs difference")
plot(control, difference, main = "control vs difference")
plot(control, test)
mean(test)
mean(control)
sd(test)
sd(control)
sd(test)
sd(control)
difference
rank(abs(difference))
par(mfrow = c(1, 2))
qqnorm(test, main = "QQ Plot for Test", xlab = "Theoretical N(0,1) Quantiles")
qqline(test)
qqnorm(control, main = "QQ Plot for Control", xlab = "Theoretical N(0,1) Quantiles")
qqline(control)
t.test(test, control, paired = TRUE, alternative = "less")
wilcox.test(test, control, paired = TRUE, alternative = "less")
group1 = c(1.7, 1.9, 6.1, 12.5, 16.5, 25.1, 30.5, 42.1, 82.5)
group2 = c(13.6, 19.8, 25.2, 46.2, 46.2, 61.1)
group3 = c(13.4, 20.9, 25.1, 29.7, 46.9)
kruskal.test(list(group1, group2, group3)
kruskal.test(list(group1, group2, group3))
kruskal.test(list(group1, group2, group3))
x = c(1,2,3,4,5,6,7)
y = c(7,6,5,4,3,2,1)
combined = rbind(x,y)
summary(combined)
combined
combined = cbind(x,y)
combined
summary(combined)
plot(combined)
plot(combined, main = 'title')
?axis
plot(combined, main = 'title', xlab = 'x', ylab = 'y')
install.packages(c('repr', 'IRdisplay', 'evaluate', 'crayon', 'pbdZMQ', 'devtools', 'uuid', 'digest'))
devtools::install_github('IRkernel/IRkernel')
pfilter
pfilter
group2 = c(13.6, 19.8, 25.2, 46.2, 46.2, 61.1)
group1 = c(1.7, 1.9, 6.1, 12.5, 16.5, 25.1, 30.5, 42.1, 82.5)
?filter
car
install.packages("dplyr")
install.packages("nycflights13")
1 + 1
print('hello world')
install.packages("nycflights13")
slice(flights, 1:10)
combined_class
set.seed(1)> x1=runif(100)> x2=0.5*x1+rnorm(100)/10> y=2+2*x1+0.3*x2+rnorm(100)
set.seed(1)
set.seed(1)
x1 = runif(100)
x2 = 0.5*x1 + rnorm(100)/10
y = 2+2*x1+0.3*x2+rnorm(100)
cor(x1,x2)
x1
x2
babynames
install.packges(babynames)
install.packages(babynames)
install.packages('babynames')
library(babynames)
library(dplyr)
babynaems
babynames
head(babynames)
babynames
library("babynames")
library("dplyr")
year.count = babynames %>%
dplyr::group_by(year) %>%
summarise(count = sum(n))
names(year.count)
head(year.count)
group_by?
asdf
?group_by
year.count
?summarise
year.count = babynames %>%
dplyr::group_by(year) %>%
year.count = babynames %>%
dplyr::group_by(year) %>%
summarise(count = sum(n))
year.count
data.frame
data.table
?summarise
babynames
t(babynames)
?mutate
?plot
library(MASS)
library(ISLR)
install.packages(ISLR)
install.packages('ISLR')
library(ISLR)
Boston
names(Boston)
lm.fit = lm(medv~lstat, data = Boston)
attach(Boston)
lm.fit
lm.fit
summary(lm.fit)
lm.fit
coef(lm.fit)
rnorm(100)
rnorm
rnorm(100)
rnorm(100, 0, 1)
confint(lm.fit)
library("babynames")
library("babyanmes")
babynames
head("babynames")
head(babynames)
head(BabyNames)
library("mdsr")
library("babynames")
install.packages("mdsr")
library("mdsr")
Minneapolis2013
head('Minneaplis')
head(Minneapolis2013)
nrow(Minneapolis2013)
tail(Minneapolis2013)
Minneapolis2013
head(Minneapolis2013)
freq(Minneapolis2013$Second)
table(Minneapolis2013$Second)
max(table(Minneapolis2013$Second))
order((table(Minneapolis2013$Second)))
sort((table(Minneapolis2013$Second)))
sort((table(Minneapolis2013$Second)), decreasing = TRUE)
head(sort((table(Minneapolis2013$Second)), decreasing = TRUE), 5
head(sort((table(Minneapolis2013$Second)), decreasing = TRUE), 5)
head(sort((table(Minneapolis2013$Second)), decreasing = TRUE), 5)
head(Minneapolis2013)
nrow(filter(Minneapolis2013, First == 'undervote'))
filter(Minneapolis2013, First == 'undervote')
head(Minneapolis2013)
unique(Minneapolis2013$Precinct)
group_by(Minneapolis2013, Precinct)
summarise(Minneapolis2013, Precinct)
summarise(Minneapolis2013, sort((table(Minneapolis2013$Second))))
summarise(Minneapolis2013, sort((table(Minneapolis2013$Precinct))))
(table(Minneapolis2013$Precinct
table(Minneapolis2013$Precinct
table(Minneapolis2013$Precinct
)
summarise(Minneapolis2013, Minneapolis2013$Precinct)
head(table(Minneapolis2013$Precinct), 1)
BabyNames %>%
group_by(BabyNames, year, sex) %>%
summarise(BabyNames, total = sum(count))
babynames
babynames %>%
group_by(year, sex) %>%
summarise(total = sum(count))
babynames %>%
group_by(year, sex) %>%
summarise(total = sum(count))
babynames %>%
group_by(year, sex) %>%
summarise(total = n())
zipgeograpy
ZipGeography
zipgeography
Minneapolis2013 %>%
group_by(First) %>%
summarise(voteReceived = n())
zipgeography %>%
group_by(State) %>%
summarise(pop = sum(Population))
Minneapolis2013 %>%
group_by(First) %>%
summarise(voteReceived = n())
group_by(Minneapolis2013, First)
head(Minneapolis2013)
babynames
arrange(babynames, sex, count)
arrange(babynames, sex)
arrange(babynames, sex, n)
filter(babynames, sex = 'F')
filter(babynames, sex == 'F')
select(babynames, name, n)
summarise(babynames, total = n())
sum(babynames$n)
summarise(babynames, total = sum(BabyNames$count))
summarise(Babynames, total = sum(babynames$count))
summarise(babynames, total = sum(babynames$count))
summarise(babynames, total = sum(nabynames$n))
summarise(babynames, total = sum(babynames$n))
summarise(babynames, total = sum(n))
babynames
install.packages("DataComputing")
library("DataComputing")
library(DataComputing)
library(dplyr)
is.na(babynames)
arrange(Minneapolis2013, First, Second)
head(arrange(Minneapolis2013, First, Second)
)
table(Minneapolis2013$First & Minneapolis2013$Second)
summarise(Minneapolis2013, First, Second)
head(arrange(Minneapolis2013, First, Second), 20)
head(arrange(Minneapolis2013, First, Second), 50)
counted = ddply(Minneapolis2013, c('First', 'Second'), nrow)
library(plyr)
counted = ddply(Minneapolis2013, c('First', 'Second'), nrow)
counted
head(counted)
sort(ddply(Minneapolis2013, c('First', 'Second'), nrow))
counted = ddply(Minneapolis2013, c('First', 'Second'), nrow)
sort(counted)
counted = ddply(Minneapolis2013, c('First', 'Second'), nrow)
counted
head(counted)
counted = counted[with(counted, order(count)),]
names(counted) = 'count'
head(counted)
counted = ddply(Minneapolis2013, c('First', 'Second'), nrow)
names(counted) = 'count'
counted
head(countd)
head(counted)
names(counted) = c('First', 'Second', 'Count')
head(counted)
sort(counted, Count)
sort(counted, counted$Count)
sort(counted$Count)
table(counted)
head(table(counted))
counted <- counted[with(counted, order(-count)),]
counted <- counted[with(counted, order(-Count)),]
head(counted)
with(counted, order(-Count))
head(counted, 1)
counted <- counted[with(counted, order(Count)),]
head(counted, 1)
counted <- counted[with(counted, order(-Count)),]
head(counted, 1)
today()
library(lubridate)
install.packages(lubridate)
install.packages('lubridate')
library(lubridate)
today()
class(today())
now()
now()
ymd("2016/02 10")
ymd("2016/0210")
ymd("2016.02 10")
date("2016/02 10")
week(today())
ymd("2015.02 10") + year(5)
ymd("2015.02 10") + years(5)
ymd("2015.02 10") + month(3)
ymd("2015.02 10") + month(20)
dminutes(60)
data=read.table("~/Downloads/s154/Assignment 2/code/parkinsons.data",header=TRUE,sep=",")
data=read.table("~/Downloads/parkinsons.data",header=TRUE,sep=",")
data = as.matrix(data[,-1])
set.seed(3041993)
data = data[sample(nrow(data)),]
status = data[,"status"]
status
data
View(data)
data = data[, -which(colnames(data)=="status") ]
dataHealthy = data[status==0,]
dataParks = data[status==1,]
nHealthy = nrow(dataHealthy)
nParks = nrow(dataParks)
data[1:2,]
trainf = 0.8
Xtrain = rbind(dataHealthy[1:floor( trainf *nHealthy ),],
dataParks [1:floor ( trainf *nParks ),])
Xtrain
View(Xtrain)
?floor
Xtrain1 = cbind(1,scale(Xtrain))
View(year.count)
View(Xtrain1)
w = solve(t(Xtrain1)%*%Xtrain1,t(Xtrain1)%*%Ttrain)
trainf = 0.8
Xtrain = rbind(dataHealthy[1:floor( trainf *nHealthy ),],
dataParks [1:floor ( trainf *nParks ),])
Ttrain = matrix(c(rep(1,floor( trainf*nHealthy)),rep(2, floor ( trainf *nParks))))
Xtest = rbind(dataHealthy[-(1:floor(trainf*nHealthy )),],
dataParks[-(1:floor ( trainf *nParks )),])
Ttest = matrix(c(rep(1,nHealthy-floor(trainf*nHealthy)),
rep(2,nParks-floor( trainf *nParks))))
#===========================================#
#LS Regression by hand
#===========================================#
#STANDARDIZE
Xtrain1 = cbind(1,scale(Xtrain))
#CALCULATE W (COEFFS FOR LS REGRESSION)
w = solve(t(Xtrain1)%*%Xtrain1,t(Xtrain1)%*%Ttrain)
w
View(w)
Xtrain1 = cbind(1,scale(Xtrain))
w = solve(t(Xtrain1)%*%Xtrain1,t(Xtrain1)%*%Ttrain)
View(w)
TtrainPredicted = Xtrain1 %*% w
TtestPredicted = cbind(1,scale(Xtest)) %*% w
pCorrectTrain = sum(apply(abs(cbind(TtrainPredicted-1, TtrainPredicted-2)),
1, which.min) == Ttrain) / length(Ttrain) * 100.0
makeQDADiscF = function(mean,sigma,prior,X) {
sigmaInv = solve(sigma)
diff= X - matrix(mean,nrow(X),ncol(X),byrow=TRUE)
return(-0.5 * log(det(sigma)) - 0.5 * rowSums(diff %*% sigmaInv * diff) + log(prior))
}
makeQDADiscF
qdadisc1tr = makeQDADiscF(mean1,cov1,nHealthy/(nHealthy+nParks),Xtrains)
mean1 = colMeans(Xtrains[Ttrain==1,])
cov1 = cov(Xtrains[Ttrain==1,])
mean2 = colMeans(Xtrains[Ttrain==2,])
cov2 = cov(Xtrains[Ttrain==2,])
N = nHealthy + nParks
covavg = cov1 * nHealthy/N + cov2 * nParks/N
#Form the QDA discriminant functions
makeQDADiscF = function(mean,sigma,prior,X) {
sigmaInv = solve(sigma)
diff= X - matrix(mean,nrow(X),ncol(X),byrow=TRUE)
return(-0.5 * log(det(sigma)) - 0.5 * rowSums(diff %*% sigmaInv * diff) + log(prior))
}
qdadisc1tr = makeQDADiscF(mean1,cov1,nHealthy/(nHealthy+nParks),Xtrains)
qdadisc2tr = makeQDADiscF(mean2,cov2,nParks/(nHealthy+nParks),Xtrains)
qdadisc1ts = makeQDADiscF(mean1,cov1,nHealthy/(nHealthy+nParks),Xtests)
qdadisc2ts = makeQDADiscF(mean2,cov2,nParks/(nHealthy+nParks),Xtests)
Xtrains = scale(Xtrain)
Xtests = scale(Xtest)
mean1 = colMeans(Xtrains[Ttrain==1,])
cov1 = cov(Xtrains[Ttrain==1,])
mean2 = colMeans(Xtrains[Ttrain==2,])
cov2 = cov(Xtrains[Ttrain==2,])
N = nHealthy + nParks
covavg = cov1 * nHealthy/N + cov2 * nParks/N
#Form the QDA discriminant functions
makeQDADiscF = function(mean,sigma,prior,X) {
sigmaInv = solve(sigma)
diff= X - matrix(mean,nrow(X),ncol(X),byrow=TRUE)
return(-0.5 * log(det(sigma)) - 0.5 * rowSums(diff %*% sigmaInv * diff) + log(prior))
}
qdadisc1tr = makeQDADiscF(mean1,cov1,nHealthy/(nHealthy+nParks),Xtrains)
qdadisc2tr = makeQDADiscF(mean2,cov2,nParks/(nHealthy+nParks),Xtrains)
qdadisc1ts = makeQDADiscF(mean1,cov1,nHealthy/(nHealthy+nParks),Xtests)
qdadisc2ts = makeQDADiscF(mean2,cov2,nParks/(nHealthy+nParks),Xtests)
qdadisc1tr
p=par(mfrow=c(1,2),bty="n")
matplot(cbind(Ttrain, TtrainPredicted ), type="b",pch=1,lty=1,
xlab="Sample",ylab="True and Predicted Class",main="Train Data")
matplot(cbind(Ttest,TtestPredicted ), type="b",pch=1,lty=1,
xlab="Sample",ylab="True and Predicted Class",main="Test Data")
par(p)
Xtrains = scale(Xtrain)
Xtrains
View(Xtrains)
[Ttrain == 1, ]
Xtrains[Ttrain == 1,]
Xtrains = scale(Xtrain)
Xtests = scale(Xtest)
mean1 = colMeans(Xtrains[Ttrain==1,])
mean1
Ttrain
cov1
View(cov1)
?solve
death_age_survivor = select(titanic, Sex, Age, Survived)
ggplot(death_age_survivor, aes(x = Sex, y = Age, col = Survived)) + geom_point()
library(DBI)
setwd("~/Desktop/Stat159")
setwd("~/Desktop/Stat159/Stat159")
setwd("~/Desktop/Stat159/Stat159/stat159-fall2016-hw02")
setwd("~/Desktop/Stat159/Stat159/stat159-fall2016-hw02/data")
data = read.csv('Advertising.csv')
data
hist(data$Sales)
